# Training Recipes Overview

FastVideo supports multiple training pathways so you can choose the right balance between data usage, compute, and convergence speed. This guide explains the three primary options—basic finetuning, bidirectional (DMD2) distillation, and causal (self-forcing/SFwan) distillation—and shows how to run each using the Wan2.1 T2V 1.3B "Basic" example as a starting point. Each section links to the autogenerated example pages for step-by-step scripts.

## Basic finetuning

Basic finetuning fully trains the backbone (optionally with LoRA adapters) on your dataset.

**When to use:** You want to adapt a model to a specific domain or style and can afford standard training costs.

**Key components**
- **Data prep:** Follow [data_preprocess.md](data_preprocess.md) to produce parquet latents.
- **Model and scripts:** The Wan2.1 T2V 1.3B crush-smol recipe ships with full and LoRA variants.
- **Reference example:** [Wan T2V 1.3B crush-smol](examples/wan_t2v_1.3B_crush_smol.md).

**Quick start (full finetune)**
1. Download the dataset: `bash examples/training/finetune/wan_t2v_1.3B/crush_smol/download_dataset.sh`
2. Preprocess to latents: `bash examples/training/finetune/wan_t2v_1.3B/crush_smol/preprocess_wan_data_t2v.sh`
3. Launch training: `bash examples/training/finetune/wan_t2v_1.3B/crush_smol/finetune_t2v.sh`

**LoRA variant**
- Swap step 3 with `bash examples/training/finetune/wan_t2v_1.3B/crush_smol/finetune_t2v_lora.sh` to train lightweight adapters while keeping the rest of the workflow the same.

## Bidirectional distillation (DMD2)

Bidirectional distillation (DMD2) trains a smaller or faster student model using guidance from a high-quality teacher while keeping sample efficiency high. It supports pure DMD as well as sparse-attention accelerated (VSA) variants.

**When to use:** You need faster inference or want to compress a teacher into a student with minimal quality loss.

**Key components**
- **Method guide:** See the DMD overview in [distillation/dmd.md](../distillation/dmd.md).
- **Model and scripts:** Wan2.1 T2V 1.3B uses the Wan-Syn 480p synthetic dataset with DMD-only and DMD+VSA launchers.
- **Reference examples:**
  - Bidirectional index: [Bidirectional distillation examples](../distillation/examples/bidirectional_distillation.md)
  - Wan2.1 T2V walkthrough: [Wan-Syn 480P recipe](../distillation/examples/Wan2.1-T2V_Wan-Syn-Data-480P.md)

**Quick start (DMD-only or DMD+VSA)**
1. Install VSA if you plan to use the VSA variant: `pip install vsa`
2. Download the dataset: `bash examples/distill/Wan2.1-T2V/Wan-Syn-Data-480P/download_dataset.sh`
3. Launch distillation:
   - DMD-only: `sbatch examples/distill/Wan2.1-T2V/Wan-Syn-Data-480P/distill_dmd_t2v.slurm`
   - DMD+VSA: `sbatch examples/distill/Wan2.1-T2V/Wan-Syn-Data-480P/distill_dmd_VSA_t2v.slurm`

## Causal distillation (Self-forcing/SFwan)

Causal distillation (self-forcing/SFwan) aligns the student by reusing its own intermediate predictions as guidance, improving stability for causal and autoregressive variants.

**When to use:** You need causal/self-forcing behavior (e.g., for autoregressive or streaming use cases) or want to leverage self-generated guidance to stabilize long-horizon sequences.

**Key components**
- **Model and scripts:** Recipes are available for SFWan2.1 T2V and SFWan2.2 A14B.
- **Reference examples:**
  - Causal index: [Causal distillation examples](../distillation/examples/causal_distillation.md)
  - SFWan2.1 T2V guide: [SFWan2.1 T2V](../distillation/examples/SFWan2.1-T2V.md)
  - SFWan2.2 A14B guide: [SFWan2.2 A14B](../distillation/examples/SFWan2.2-A14B.md)

**Quick start**
1. Download or prepare your dataset as described in each example page (e.g., `bash examples/distill/SFWan2.1-T2V/download_dataset.sh`).
2. Run the provided launchers for your target model:
   - SFWan2.1 T2V: `sbatch examples/distill/SFWan2.1-T2V/distill_dmd_t2v_1.3B.sh`
   - SFWan2.2 A14B: `bash examples/distill/SFWan2.2-A14B/distill_dmd.sh`

## Where to go next
- Browse all autogenerated training examples: [Training examples index](examples/examples_training_index.md)
- Check inference pipelines after training: [Inference examples index](../inference/examples/examples_inference_index.md)
