[1mdiff --git a/fastvideo/pipelines/composed_pipeline_base.py b/fastvideo/pipelines/composed_pipeline_base.py[m
[1mindex 65d44116..ca53f69c 100644[m
[1m--- a/fastvideo/pipelines/composed_pipeline_base.py[m
[1m+++ b/fastvideo/pipelines/composed_pipeline_base.py[m
[36m@@ -79,7 +79,7 @@[m [mclass ComposedPipelineBase(ABC):[m
             for name, module in self.modules.items():[m
                 if not isinstance(module, torch.nn.Module):[m
                     continue[m
[31m-                if name == "transformer":[m
[32m+[m[32m                if "transformer" in name:[m
                     module.requires_grad_(True)[m
                 else:[m
                     module.requires_grad_(False)[m
[1mdiff --git a/fastvideo/pipelines/lora_pipeline.py b/fastvideo/pipelines/lora_pipeline.py[m
[1mindex 8da8d2c8..726a02aa 100644[m
[1m--- a/fastvideo/pipelines/lora_pipeline.py[m
[1m+++ b/fastvideo/pipelines/lora_pipeline.py[m
[36m@@ -93,6 +93,7 @@[m [mclass LoRAPipeline(ComposedPipelineBase):[m
             self.modules["fake_score_transformer"].requires_grad_(False)[m
         device_mesh = init_device_mesh("cuda", (dist.get_world_size(), 1),[m
                                        mesh_dim_names=["fake", "replicate"])[m
[32m+[m[32m        torch.distributed.breakpoint()[m
         for name, layer in (self.lora_layers | self.lora_layers_critic).items():[m
             # Enable grads for lora weights only[m
             # Must convert to DTensor for compatibility with other FSDP modules in grad calculation[m
[1mdiff --git a/fastvideo/training/distillation_pipeline.py b/fastvideo/training/distillation_pipeline.py[m
[1mindex fa3a6fcd..4c294c1c 100644[m
[1m--- a/fastvideo/training/distillation_pipeline.py[m
[1m+++ b/fastvideo/training/distillation_pipeline.py[m
[36m@@ -905,7 +905,7 @@[m [mclass DistillationPipeline(TrainingPipeline):[m
             count_trainable(self.fake_score_transformer) / 1e9, 3)[m
         logger.info([m
             "rank: %s: # of trainable params in generator: %sB, # of trainable params in critic: %sB",[m
[31m-            self.global_rank, num_trainble_generator, num_trainble_critic)[m
[32m+[m[32m            self.global_rank, num_trainable_generator, num_trainable_critic)[m
         # Set random seeds for deterministic training[m
         self.noise_random_generator = torch.Generator(device="cpu").manual_seed([m
             self.seed)[m
